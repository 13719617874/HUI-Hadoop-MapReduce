# Hadoop-MapReduce
项目简介
基于 Hadoop 生态，设计并实现了一个大数据词频统计与分析系统，涵盖数据采集、存储、处理与分析的全流程。项目通过 MapReduce、Hive、Flume、Sqoop 等组件，
实现了对大规模文本数据的高效处理与分析，并支持与传统关系型数据库的数据交互。

核心技术与职责
MapReduce 部分
熟练使用 Java 编写 Hadoop MapReduce 程序，实现大规模文本数据的分布式词频统计。
能独立设计 Mapper/Reducer 逻辑，完成数据的分割、聚合与最终输出。
掌握 MapReduce 作业的配置、参数设置与任务提交流程。

Hive 部分
能编写 Hive SQL 脚本，完成外部表的创建、数据导入与批量数据分析。
熟悉 Hive 表的分隔符、存储路径等底层实现细节。
能用 SQL 实现高效的分组、聚合与排序操作，满足大数据分析需求。
使用 MapReduce 编写 Java 程序，实现分布式文本数据的词频统计，提升数据处理效率。
利用 Hive 进行数据建模与批量 SQL 查询，简化大数据分析流程。
配置 Flume 实现对 Linux 系统本地日志/文本的实时采集与传输，保障数据流转的实时性与可靠性。
通过 Sqoop 实现 Hive 与 MySQL 之间的数据导入导出，打通大数据平台与传统数据库的壁垒。
熟悉 Hadoop 生态各组件的集成与调优，具备大数据平台的实际操作经验。

项目亮点
覆盖大数据采集、存储、处理、分析、导出的完整链路。
具备分布式计算、SQL 分析、实时数据采集、数据迁移等多项能力。
项目文档齐全，代码规范，易于维护和扩展。
熟悉 Linux 环境下的大数据平台部署与运维。
能独立完成大数据 ETL 流程的设计与实现。
具备良好的团队协作与文档编写能力。
